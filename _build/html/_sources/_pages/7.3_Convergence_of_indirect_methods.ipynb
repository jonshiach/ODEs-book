{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b64878ec",
   "metadata": {},
   "source": [
    "(convergence-of-indirect-methods-section)=\n",
    "\n",
    "# Convergence of indirect methods\n",
    "\n",
    "We have seen that both the [Jacobi](jacobi-method-section) and [Gauss-Seidel](gauss-seidel-method-section) methods are iterated until the estimates of the solution converge to a given tolerance. In {prf:ref}`jacobi-method-example` required 49 iterations to converge to the solution of a system of linear equations whereas in {prf:ref}`gauss-seidel-method-example` only required 20 iterations to converge to the solution for the same system. \n",
    "\n",
    "Not all indirect methods will converge for a given system of linear equations, we can establish whether a method will be convergent using the theorem below. Let $\\vec{e}^{(k)} = \\|\\vec{x} - \\vec{x}^{(k)}\\|$ be the error between the exact solution $\\vec{x}$ and the estimate $\\vec{x}^{(k)}$. The error from one estimate to the next is updated using the iteration matrix for the method\n",
    "\n",
    "$$ \\vec{e}^{(k+1)} = T\\vec{e}^{(k)}. $$\n",
    "\n",
    "We can write the first error, $\\vec{e}^{(0)}$ as a linear combination of some vectors $\\vec{v}_i$ \n",
    "\n",
    "$$ \\vec{e}^{(0)} =\\alpha_1 \\vec{v}_1 +\\alpha_2 \\vec{v}_2 +\\cdots +\\alpha_n \\vec{v}_n =\\sum_{i=1}^n \\alpha_i \\vec{v}_i, $$\n",
    "\n",
    "where $\\alpha_i$ are scalars. If $\\vec{v}_i$ are the <a href=\"https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors\" target=\"_blank\">eigenvalues</a> of the iteration matrix $T$ with eigenvalues $\\lambda_i$ so $T\\vec{v}_i = \\lambda_i \\vec{v}_i$ then iterating $\\vec{e}^{(k)}$ gives\n",
    "\n",
    "$$ \\begin{align*}\n",
    "    \\vec{e}^{(1)} &= T\\vec{e}^{(0)} =T\\left(\\sum_{i=1}^n \\alpha_i \\vec{v}_i \\right)=\\sum_{i=1}^n \\alpha_i T\\vec{v}_i = \\sum_{i=1}^n \\alpha_i \\lambda_i \\vec{v}_i , \\\\\n",
    "    \\vec{e}^{(2)} &=T\\vec{e}^{(1)} =T\\left(\\sum_{i=1}^n \\alpha_i \\lambda_i \\vec{v}_i \\right)=\\sum_{i=1}^n \\alpha_i \\lambda_i T\\vec{v}_i =\\sum_{i=1}^n \\alpha_i \\lambda_i^2 \\vec{v}_i , \\\\\n",
    "    &\\vdots \\\\\n",
    "    \\vec{e}^{(k+1)} &=\\sum_{i=1}^n \\alpha_i \\lambda_i^{k+1} \\vec{v}_i .\n",
    "\\end{align*} $$\n",
    "\n",
    "If $|\\lambda_1|>\\lambda_2, \\lambda_3, \\ldots \\lambda_n$ then\n",
    "\n",
    "$$ \\vec{e}^{(k+1)} =\\alpha_1 \\lambda_1^{k+1} \\vec{v}_1 +\\sum_{i=2}^n \\alpha_i \\lambda_i^{k+1} \\vec{v}_i =\\lambda_1^{k+1} \\left(\\alpha_1 \\vec{v}_1 +\\sum_{i=2}^n \\alpha_i \\vec{v}_i {\\left(\\frac{\\lambda_i }{\\lambda_1 }\\right)}^{k+1} \\right) $$\n",
    "\n",
    "and since $\\lambda_i / \\lambda_ 1 < 1$ then\n",
    "\n",
    "$$ \\begin{align*}\n",
    "    \\lim_{k\\to \\infty } \\vec{e}^{(k+1)} =\\alpha_1 \\lambda_1^{k+1} \\vec{v}_1 .\n",
    "\\end{align*} $$\n",
    "\n",
    "This means that as the number of iterations increases, the error varies by a factor of $\\lambda_1^{k+1}$ where $\\lambda_1$ is the largest eigenvalue of $T$ which is known as the **spectral radius**.\n",
    "\n",
    "```{prf:definition} Spectral radius\n",
    ":label: spectral-radius-definition\n",
    "\n",
    "The spectral radius of $A$ denoted by $\\rho(A)$ is\n",
    "\n",
    "$$ \\rho(A) = \\max_i(| \\lambda_i |). $$(spectral-radius-equation)\n",
    "\n",
    "where $\\lambda_i$ are the eigenvalues of $A$.\n",
    "```\n",
    "\n",
    "```{prf:theorem} Convergence criteria for an indirect method\n",
    ":label: indirect-methods-convergence-criteria-theorem\n",
    "\n",
    "The spectral radius of $T$ gives us the following information about an indirect method\n",
    "\n",
    "- If $\\rho (T) > 1$ then the errors will increase over each iteration, therefore for an indirect method to converge to the solution we require $\\rho (T)< 1$.\n",
    "- The smaller the value of $\\rho (T)$ the faster the errors will tend to zero.\n",
    "```\n",
    "\n",
    "````{prf:example}\n",
    ":label: convergence-example\n",
    "\n",
    "Show that the Jacobi and Gauss-Seidel methods are convergent of the system of linear equations from {prf:ref}`jacobi-method-example` (shown below)\n",
    "\n",
    "$$ \\begin{align*}\n",
    "    x_{1}^{(k+1)} &= \\frac{1}{4} \\left( -2 - 3 x_{2}^{(k)} \\right), \\\\\n",
    "    x_{2}^{(k+1)} &= \\frac{1}{4} \\left( -8 - 3 x_{1}^{(k)} + x_{3}^{(k)} \\right), \\\\\n",
    "    x_{3}^{(k+1)} &= \\frac{1}{4} \\left( 14 + x_{2}^{(k)} \\right).\n",
    "\\end{align*} $$\n",
    "\n",
    "```{dropdown} Solution (click to show)\n",
    "\n",
    "The coefficient matrix for this linear system is\n",
    "\n",
    "$$ A = \\begin{pmatrix} 4 & 3 & 0 \\\\ 3 & 4 & -1 \\\\ 0 & -1 & 4 \\end{pmatrix}. $$\n",
    "\n",
    "so\n",
    "\n",
    "$$ \\begin{align*}\n",
    "    L &= \\begin{pmatrix} 0 & 0 & 0 \\\\ 3 & 0 & 0 \\\\ 0 & -1 & 0 \\end{pmatrix}, &\n",
    "    D &= \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix}, \\\\\n",
    "    U &= \\begin{pmatrix} 0 & 3 & 0 \\\\ 0 & 0 & -1 \\\\ 0 & 0 & 0 \\end{pmatrix}.\n",
    "\\end{align*} $$\n",
    "\n",
    "The iteration matrices for the Jacobi and Gauss-Seidel methods are given in equations {eq}`jacobi-method-iteration-matrix-equation` and {eq}`gauss-seidel-method-iteration-matrix-equation` which for this system are\n",
    "\n",
    "$$ \\begin{align*}\n",
    "    T_J &= -D^{-1} ( L + U)  \\\\\n",
    "    &= -\\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} ^{-1} \\left(\n",
    "    \\begin{pmatrix} 0 & 0 & 0 \\\\ 3 & 0 & 0 \\\\ 0 & -1 & 0 \\end{pmatrix} +\n",
    "    \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} \\right) \\\\\n",
    "    &= - \\begin{pmatrix} \\frac{1}{4} & 0 & 0 \\\\ 0 & \\frac{1}{4} & 0 \\\\ 0 & 0 & \\frac{1}{4} \\end{pmatrix}\n",
    "    \\begin{pmatrix} 0 & 3 & 0 \\\\ 3 & 0 & -1 \\\\ 0 & -1 & 0 \\end{pmatrix} \\\\ \n",
    "    &= \\begin{pmatrix} 0 & -\\frac{3}{4} & 0 \\\\ -\\frac{3}{4} & 0 & \\frac{1}{4} \\\\ 0 & \\frac{1}{4} & 0 \\end{pmatrix}, \\\\\n",
    "    T_{GS} &= - (L + D)^{-1} U  \\\\\n",
    "    &= -\\left( \n",
    "        \\begin{pmatrix} 0 & 0 & 0 \\\\ 3 & 0 & 0 \\\\ 0 & -1 & 0 \\end{pmatrix} +\n",
    "        \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix}\n",
    "    \\right)^{-1}\n",
    "    \\begin{pmatrix} 0 & 3 & 0 \\\\ 0 & 0 & -1 \\\\ 0 & 0 & 0 \\end{pmatrix}\n",
    "    \\\\\n",
    "    &= - \\begin{pmatrix} 3 & 0 & 0 \\\\ 3 & 4 & 0 \\\\ 0 & -1 & 4 \\end{pmatrix}^{-1}\n",
    "    \\begin{pmatrix} 0 & 3 & 0 \\\\ 0 & 0 & -1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\\\ \n",
    "    &= \\begin{pmatrix} -\\frac{1}{4} & 0 & 0 \\\\ \\frac{3}{16} & \\frac{1}{4} & 0 \\\\ \\frac{3}{64} & -\\frac{1}{16} & \\frac{1}{4} \\end{pmatrix}\n",
    "    \\begin{pmatrix} 0 & 3 & 0 \\\\ 0 & 0 & -1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\\\\n",
    "    & =\n",
    "    \\begin{pmatrix} 0 & -\\frac{3}{4} & 0 \\\\ 0 & \\frac{9}{16} & \\frac{1}{4} \\\\ 0 & \\frac{9}{64} & \\frac{1}{16} \\end{pmatrix}\n",
    "\\end{align*} $$\n",
    "\n",
    "Calculating the spectral radius for these iteration matrices gives $\\rho(T_J )=0.7906$ and $\\rho (T_{GS})=0.625$ which are both less than 1 so both of these methods are convergent for this system. Furthermore, the Gauss-Seidel method will converge faster than the Jacobi method since it has a smaller spectral radius.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c98e8f0f",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho(TJ) = {-sqrt(10)/4: 1, sqrt(10)/4: 1, 0: 1}\n",
      "rho(TGS) = {5/8: 1, 0: 2}\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "def jacobi_iteration_matrix(A):\n",
    "    L = A.lower_triangular(-1)\n",
    "    U = A.upper_triangular(1)\n",
    "    D = A - L - U\n",
    "    return - D.inv() * (L + U)\n",
    "\n",
    "\n",
    "def gauss_seidel_iteration_matrix(A):\n",
    "    L = A.lower_triangular(-1)\n",
    "    U = A.upper_triangular(1)\n",
    "    D = A - L - U\n",
    "    return - (L + D).inv() * U\n",
    "\n",
    "\n",
    "# Define matrix\n",
    "A = sp.Matrix([[4, 3, 0], [3, 4, -1], [0, -1, 4]])\n",
    "\n",
    "# Calculate iteration matrices\n",
    "TJ = jacobi_iteration_matrix(A)\n",
    "TGS = gauss_seidel_iteration_matrix(A)\n",
    "\n",
    "# Calculate eigenvalues of iteration matrices\n",
    "rhoTJ = TJ.eigenvals()\n",
    "rhoTGS = TGS.eigenvals()\n",
    "\n",
    "print(f\"rho(TJ) = {rhoTJ}\")\n",
    "print(f\"rho(TGS) = {rhoTGS}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "3a8893f41ea6eea20a2f9e20d8f8fd0ea69174c942b987ace920455cbb0cf5de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
