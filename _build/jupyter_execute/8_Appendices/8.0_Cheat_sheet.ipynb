{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c12771",
   "metadata": {},
   "source": [
    "# Cheat Sheet\n",
    "\n",
    "## Initial value problems\n",
    "\n",
    "\n",
    "````{glossary}\n",
    "\n",
    "Ordinary Differential Equation (ODE)\n",
    "    An [ODE](ode-section) is an equation of the for \n",
    "    \n",
    "    $$y^{(n)} = f(t, y, y', y'', \\ldots, y^{(n-1)}),$$\n",
    "    \n",
    "    where $y(t)$ is a function of the independent variable $t$. \n",
    "\n",
    "Initial Value Problem (IVP)    \n",
    "    An [initial value problem](ivp-definition) is an ODE where the solution at some initial state is known\n",
    "    \n",
    "    $$y' = f(t, y), \\qquad t \\in [a, b], \\qquad y(a) = \\alpha.$$\n",
    "\n",
    "The Taylor series\n",
    "    The [Taylor series](taylor-series-section) is a series expansion of a function $f(t)$\n",
    "    \n",
    "    $$f(t + h) = \\sum_{n=0}^\\infty \\frac{h^n}{n!}f^{(n)}(t).$$\n",
    "\n",
    "Truncating the Taylor series\n",
    "    [Truncating the Taylor series](taylor-series-section) is where all terms after the $n$th derivative in the Taylor series are omitted. \n",
    "    \n",
    "    $$f(t + h) \\approx f(t) + hf'(t) + \\frac{h^2}{2!} f''(t) + \\cdots + \\frac{h^n}{n!} f^{(n)}(t).$$\n",
    "\n",
    "The Euler method\n",
    "    The [Euler method](euler-method-section) is a first-order accurate numerical method for solving an initial value problem of the form $y' = f(t, y)$, $t\\in [a, b]$, $t_0 = \\alpha$\n",
    "    \n",
    "    $$y_{n+1} = y_n + h f(t_n, y_n),$$\n",
    "    \n",
    "    $t_n$ is a value of $t$ at step $n$, $y_n = y(t_n)$ and $h = t_{n+1} - t_n$ is the step length.\n",
    "    \n",
    "Local Truncation Error (LTE)\n",
    "    The [local truncation error](local-truncation-error-section) is the error in the calculation of a single step of a numerical method assuming that the previous values are exact.\n",
    "    \n",
    "Global Truncation Error (GTE)\n",
    "    The [global truncation error](global-truncation-error-section) is the error that has accumulated over all previous steps of a numerical method assuming the initial solution was known to be exact.\n",
    "    \n",
    "Big-O notation\n",
    "    $f(h) = O(h^n)$ means that as $h \\to 0$ then $f(h) \\to 0$ at least as fast as $h^n \\to 0$.\n",
    "\n",
    "Order of a method\n",
    "    The [order](global-truncation-error-section) of a method is the degree of $O(h^n)$ for the global truncation error of the method.\n",
    "    \n",
    "Reducing higher order ODE to first-order ODEs\n",
    "    An $n$th order ODE $y^{(n)} = f(t, y, y', y'', \\ldots, y^{(n-1)}$ can be written as a system of $n$ first order ODES such that\n",
    "    \n",
    "    \\begin{align*}\n",
    "        y_1' &= y_2, \\\\\n",
    "        y_2' &= y_3, \\\\\n",
    "        & \\vdots \\\\\n",
    "        y_n' &= f(t, y_1, y_2, \\ldots, y_n).\n",
    "    \\end{align*}\n",
    "````\n",
    "---\n",
    "\n",
    "## Explicit Runge-Kutta methods\n",
    "\n",
    "````{glossary}\n",
    "\n",
    "General form of a Runge-Kutta method\n",
    "    The [general form of a Runge-Kutta method](general-form-of-a-RK-method-section) is\n",
    "    \n",
    "    \\begin{align*}\n",
    "        y_{n+1} &= y_n + h\\sum_{i=1}^s b_i k_i, \\\\\n",
    "        k_i &= f(t_n + c_ih, y_n + h \\sum_{j=1}^s a_{ij} k_i,\n",
    "    \\end{align*}\n",
    "    \n",
    "    where $k_i$ are intermediate stage valeus.\n",
    "    \n",
    "Butcher tableau\n",
    "    Runge-Kutta methods can be represented using a [Butcher tableau](butcher-tableau-section) such that\n",
    "    \n",
    "    \\begin{align*}\n",
    "        \\begin{array}{c|c}\n",
    "            \\mathbf{c} & A \\\\ \\hline\n",
    "            & \\mathbf{b}^\\mathrm{T}\n",
    "        \\end{array}\n",
    "    \\end{align*}\n",
    "    \n",
    "    where $[A]_{ij} = a_{ij}$, $\\mathbf{b} = (b_1, \\ldots, b_s)^\\mathrm{T}$ and $\\mathbf{c} = (c_1, \\ldots, c_s)^\\mathrm{T}$.\n",
    "    \n",
    "Explicit Runge-Kutta method\n",
    "    An [explicit Runge-Kutta method](explicit-and-implicit-rk-methods-section) is a method where $c_1 = 0$, $a_{ij} = 0$ where $i \\leq j$ such that the stage values are explicit functions.\n",
    "    \n",
    "Order conditions of a Runge-Kutta method\n",
    "    Comparing the $n$th order Taylor series expansion of $y' = f(t, y)$ to that of the general form of a Runge-Kutta method gives the [order conditions](rk2-derivation-section) which need to be satisfied for an $n$th order Runge-Kutta method.\n",
    "  \n",
    "The row sum condition\n",
    "    The values of $a_{ij}$ and $c_i$ must satisfy the [row sum condition](row-sum-condition) which is $c_i = \\displaystyle_{j=1}^s a_{ij}$.\n",
    "    \n",
    "The second-order explicit Runge-Kutta method (RK2)\n",
    "    *The* [second-order explicit Runge-Kutta method](rk2-derivation-example) has the Butcher tableau\n",
    "    \n",
    "    \\begin{align*}\n",
    "        \\begin{array}{c|cc}\n",
    "            0 & 0 \\\\ \n",
    "            1 & 1 \\\\ \\hline\n",
    "            & \\frac{1}{2} & \\frac{1}{2}\n",
    "        \\end{array}\n",
    "    \\end{align*}\n",
    "    \n",
    "The fourth-order explicit Runge-Kutta method (RK4)\n",
    "    *The* [fourth-order explicit Runge-Kutta method](rk4-definition) has the Butcher tableau\n",
    "    \n",
    "    \\begin{align*}\n",
    "        \\begin{array}{c|cccc}\n",
    "            0 & 0 \\\\\n",
    "            \\frac{1}{2} & \\frac{1}{2} \\\\\n",
    "            \\frac{1}{2} & 0 & \\frac{1}{2} \\\\ \n",
    "            1 & 0 & 0 & 1 \\\\ \\hline\n",
    "            & \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{6}\n",
    "        \\end{array}\n",
    "    \\end{align*}\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5de14c",
   "metadata": {},
   "source": [
    "---\n",
    "## Implicit Runge-Kutta methods\n",
    "\n",
    "````{glossary}\n",
    "\n",
    "Implicit Runge-Kutta methods (IRK)\n",
    "    An [implicit Runge-Kutta method](irk-chapter) is where the stage values are implicit functions.\n",
    "    \n",
    "Order of an implicit Runge-Kutta method\n",
    "    Implicit Runge-Kutta methods can have a higher order of accuracy for the same number of stages as an explicit Runge-Kutta method. The [order of an implicit Runge-Kutta methods](order-of-irk-section) is the highest value of $k$ for which the order conditions $B(k)$, $C(\\lfloor \\frac{k}{2} \\rfloor)$ and $D(\\lfloor \\frac{k}{2} \\rfloor)$ are satisfied where\n",
    "    \n",
    "    \\begin{align*}\n",
    "        B(k): && \\sum_{i=1}^s b_i c_i^{j-1} = & \\frac{1}{j}, & j&=1\\ldots k, \\\\\n",
    "        C(k): && \\sum_{j=1}^s a_{ij} c_j^{\\ell-1} = & \\frac{1}{\\ell}c_i^{\\ell} , & i&=1 \\ldots s, & \\ell &=1 \\ldots k,\\\\\n",
    "        D(k): && \\sum_{i=1}^s b_i c_i^{\\ell-1} a_{ij} = & \\frac{1}{\\ell}b_j (1-c_j^{\\ell}), & j&=1 \\ldots s, & \\ell &=1 \\ldots k.\n",
    "    \\end{align*}\n",
    "    \n",
    "Gauss-Legendre implicit Runge-Kutta methods\n",
    "    An $s$-stage [Gauss-Legendre](gauss-legendre-derivation) implicit Runge-Kutta method has order $k = 2s$ and are derived by setting $c_i$ to be the roots of the Legendre polynomial \n",
    "    \n",
    "    $$P_n(t) = \\displaystyle\\sum_{k=0}^n \\binom{n}{k} \\binom{n+k}{k} (t - 1)^k,$$\n",
    "    \n",
    "    and $b_i$ and $a_{ij}$ are chosen to satisfy the $B(k)$ and $C(\\lfloor \\frac{k}{2} \\rfloor)$ order conditions. For \n",
    "    \n",
    "Radau IA implicit Runge-Kutta method\n",
    "    An $s$-stage [Radau IA](radau-derivation) implicit Runge-Kutta method has order $k = 2s - 1$ and are derived by setting $c_i$ to be the roots of $P_s(t) + P_{s-1}(t)$ and the values of $b_i$ and $a_{ij}$ are chosen to satisfy the order condition $D(k)$.\n",
    "    \n",
    "Radau IIA implicit Runge-Kutta method \n",
    "    An $s$-stage [Radau IIA](radau-derivation) implicit Runge-Kutta method has order $k = 2s - 1$  are derived by setting $c_i$ to be the roots of $P_s(t) - P_{s-1}(t)$ and the values of $b_i$ and $a_{ij}$ are chosen to satisfy the order condition $C(k)$.\n",
    "    \n",
    "Diagonally Implicit Runge-Kutta (DIRK) methods\n",
    "    An [DIRK](dirk-derivation) method is an implicit Runge-Kutta method where $a_{ij} = 0$ where $i < j$ and is derived by satisfying the $B(k)$ and $C(\\lfloor \\frac{k}{2} \\rfloor)$ and \n",
    "    \n",
    "    $$\\mathbf{b}^T A \\mathbf{c} = \\frac{1}{k!}.$$\n",
    "    \n",
    "    The stage values of a DIRK method can be calculated sequentially.\n",
    "    \n",
    "Singly Diagonally Implicit Runge-Kutta (SDIRK) methods\n",
    "    An [SDIRK](sdirk-derivation) method an implicit Runge-Kutta method where $a_{ij} = 0$ where $i < j$ and is derived by satisfying the $B(k)$, $C(\\lfloor \\frac{k}{2} \\rfloor)$ and $D(\\lfloor \\frac{k}{2} \\rfloor)$ order conditions.\n",
    "    \n",
    "Computing the stage values of an implicit Runge-Kutta method\n",
    "    [Computing the stage values](solving-ivps-using-irk-methods-section) of an implicit Runge-Kutta method requires the solution to a system of nonlinear equations. This is typically done using Newton's method but can also be done using the Gauss-Seidel method.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb5751b",
   "metadata": {},
   "source": [
    "---\n",
    "## Stability\n",
    "\n",
    "````{glossary}\n",
    "\n",
    "Stability\n",
    "    A method is [stable](stability-definition) if the local truncation errors do not increase from one step to the next.\n",
    "    \n",
    "Stiffness\n",
    "    An ODE is [stiff](stiffness-section) if it requires a very small value of the step length $h$ in order for a method to remain stable.\n",
    "    \n",
    "Stiffness ratio\n",
    "    The [stiffness ratio](stiffness-ratio-definition) gives a measure to whether an system of ODEs is stiff and is calculated using\n",
    "    \n",
    "    \\begin{align*}\n",
    "        S = \\frac{\\max_i(|\\lambda_i|)}{\\min_i(|\\lambda_i|)}.\n",
    "    \\end{align*}\n",
    "    \n",
    "    If $S$ is large then the system is considered stiff.\n",
    "    \n",
    "Test ODE\n",
    "    The stability of numerical methods for solving ODEs can be analysed by considering the simple [test ODE](stability-functions-section) $y' = \\lambda \\lambda y$.\n",
    "\n",
    "Stability function\n",
    "    The [stability function](stability-function-definition) $R(z)$ for a method is the change of the solution over a single step of the method when used to solve the test ODE\n",
    "    \n",
    "    $$y_{n+1} = R(z) y_n.$$\n",
    "    \n",
    "Absolute stability\n",
    "    A method is considered [absolutely stable](absolute-stability-section) if $|R(z)| \\leq 1$ for $z \\in \\mathbb{C}$.\n",
    "    \n",
    "Region of absolute stability\n",
    "    The [region of absolute stability](region-of-absolute-stability-definition) is the set of values of $z$ for which the method is absolutely stable. \n",
    "    \n",
    "Interval of absolute stability\n",
    "    The [interval of absolute stability](interval-of-absolute-stability-section) is the range of values of $h$ for which a method is absolutely stable.\n",
    "\n",
    "Stability function of an explicit Runge-Kutta method\n",
    "    The [stability function of an explicit Runge-Kutta method](erk-rz-section) can be determined using\n",
    "    \n",
    "    \\begin{align*}\n",
    "        R(z)=1+\\sum_{k=0}^{\\infty} \\mathbf{b}^T A^k \\mathbf{e}\\,z^{k+1} =1+\\mathbf{b}^T \\mathbf{e}\\,z+\\mathbf{b}^T A\\mathbf{e}\\,z^2 +\\mathbf{b}^T A^2 \\mathbf{e}\\,z^3 + \\cdots\n",
    "    \\end{align*}\n",
    "    \n",
    "Order of an explicit Runge-Kutta method\n",
    "    The order of an explicit Runge-Kutta method is the order of the highest term in the stability function for that method that agrees with the series expansion of $e^z$\n",
    "    \n",
    "    \\begin{align*}\n",
    "        e^z = \\sum_{k=0}^{\\infty} \\frac{1}{k!} z^k = 1 + z + \\frac{1}{2!}z^2 + \\frac{1}{3!}z^3 + \\frac{1}{4!}z^4 + \\cdots\n",
    "    \\end{align*}\n",
    "    \n",
    "Stability function of an implicit Runge-Kutta method\n",
    "    The [stability function of an implicit Runge-Kutta method](implicit-rz-section) can be determined using\n",
    "    \n",
    "    \\begin{align*}\n",
    "        R(z) = \\frac{\\det (I - zA + z\\mathbf{e}\\mathbf{b}^T)}{\\det(I - zA)}.\n",
    "    \\end{align*}\n",
    "    \n",
    "A-stability\n",
    "    An implicit method is considered [A-stable](A-stability) if its region of absolute stability includes all points on the left-hand side of the complex plane.\n",
    "    \n",
    "Conditions for A-stability\n",
    "    For a method to be A-stable it must satisfied the following [conditions](a-stability-theorem)\n",
    "    \n",
    "    - All roots of the denominator of $R(z)$ have positive real parts\n",
    "    - $E(y)=Q(iy)Q(-iy)-P(iy)P(-iy) \\geq 0$ for all $y\\in \\mathbb{R}$\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93453121",
   "metadata": {},
   "source": [
    "---\n",
    "## Boundary Value Problems\n",
    "\n",
    "```{glossary}\n",
    "\n",
    "Two point Boundary Value Problem (BVP)\n",
    "    A [two point boundary value problem](bvp-section) is an ODE where the solutions at the lower and upper boundaries of the domain are known\n",
    "    \n",
    "    $$y'' = f(t, y), \\qquad t \\in [a, b], \\qquad y(a) = \\alpha, \\qquad y(b) = \\beta.$$\n",
    "    \n",
    "Uniqueness of solutions to a BVP\n",
    "    A BVP may have a [unique solution, infinitely many solutions or no solutions](existence-and-uniqueness-of-bvp-solutions-section). A BVP of the form $y'' = p(t) y' + q(t) y + r(t)$ has a unique solution if the following conditions are satisfied\n",
    "    \n",
    "    - $p(t)$, $q(t)$ and $r(t)$ are continuous on $[a, b]$;\n",
    "    - $q(t) > 0$ for all $t\\in [a,b]$.\n",
    "    \n",
    "The shooting method\n",
    "    The [shooting method](shooting-method-section) for solving a BVP uses estimates of the initial value of $y'(a)$ and compares the numerical solution for $y(b)$ to the known solution $y(b)=\\beta$ and changes the estimates accordingly. \n",
    "    \n",
    "The secant method\n",
    "    The [secant method](secant-method-section) is a root finding method that can be used to calculate improved estimates in the shooting method. If $s_{i}$ and $s_{i-1}$ are current and previous guess values of $y'(a)$ then an improved guess value is\n",
    "    \n",
    "    \\begin{align*}\n",
    "        s_{i+1} = s_i - g(s_i)\\frac{s_i - s_{i-1}}{g(s_i) - g(s_{i-1})}.\n",
    "    \\end{align*}\n",
    "    \n",
    "    where $g(s) = y'(a) - y_n$ and $y_n$ is the numerical solution for $y(b)$. \n",
    "    \n",
    "Finite-difference approximations\n",
    "    A [finite-difference approximation](finite-difference-method-section) is an approximation of a derivative derived from the Taylor series.\n",
    "\n",
    "Finite-difference methods\n",
    "    The [finite-difference method](finite-difference-method-section) uses finite-difference approximations to approximate a BVP using a system of linear equations (usually tridiagonal).\n",
    "   \n",
    "The Thomas algorithm\n",
    "    The [Thomas algorithm](thomas-algorithm-section) is an efficient method for solving tridiagonal systems of linear equations.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13718e5b",
   "metadata": {},
   "source": [
    "---\n",
    "## Direct methods\n",
    "\n",
    "```{glossary}\n",
    "\n",
    "System of linear equations\n",
    "    A [system of linear equations](systems-of-linear-equations-section) is a set of linear equations of the form $A \\mathbf{x} = \\mathbf{b}$ where the solution vector $\\mathbf{x}$ satisfies all equations in the system.\n",
    "    \n",
    "Direct methods\n",
    "    [Direct methods](direct-methods-chapter) solve a system of linear equations using a single application of the method.\n",
    "    \n",
    "LU decomposition\n",
    "    [LU decomposition](lu-section) factorises a square matrix $A$ into a lower and upper triangular matrix $L$ and $U$ such that $A = LU$\n",
    "    \\begin{align*}\n",
    "        u_{ij} &= a_{ij} - \\sum_{k=1}^{i-1} \\ell_{ik}u_{kj}, & i &= 1, \\ldots, j, \\\\\n",
    "        \\ell_{ij} &= \\dfrac{1}{u_{jj}} \\left(a_{ij} - \\displaystyle \\sum_{k=1}^{j-1} \\ell_{ik}u_{kj}\\right), & i &= j+1, \\ldots, n.\n",
    "    \\end{align*}\n",
    " \n",
    "Crout's method\n",
    "    The solution of a system of linear equations $A \\mathbf{x} = \\mathbf{b}$ can be calculated using [Crout's method](crouts-method-section) where $L \\mathbf{y} = \\mathbf{b}$ is solved for $\\mathbf{y}$ by forward substitution and $U \\mathbf{x} = \\mathbf{y}$ is solved for $\\mathbf{x}$ by back substitution.\n",
    "    \n",
    "Partial pivoting\n",
    "    [Partial pivoting](partial-pivoting-section) uses row swaps to ensure that the pivot elements $a_{jj}$ have a larger absolute value that the elements in the column beneath it. \n",
    "    \n",
    "Permutation matrix\n",
    "    The permutation matrix is calculated by performing the same row swaps as used in partial pivoting to the identity matrix. \n",
    "    \n",
    "LUP decomposition\n",
    "    [LUP decomposition](lup-decomposition-section) factorises a square matrix $A$ into a lower and upper triangular matrices $L$ and $U$ and the permutation matrix $P$ such that $PA = LU$.\n",
    "    \n",
    "Crout's method with LUP decomposition\n",
    "    The solution of a system of linear equations $A \\mathbf{x} = \\mathbf{b}$ can be calculated using [Crout's method](lup-decomposition-section) where $L \\mathbf{y} = P \\mathbf{b}$ is solved for $\\mathbf{y}$ by forward substitution and $U \\mathbf{x} = \\mathbf{y}$ is solved for $\\mathbf{x}$ by back substitution.\n",
    "    \n",
    "Positive definite matrix\n",
    "    A matrix is [positive definite](cholesky-section) if it is symmetric and the determinants of all upper left square matrices are all positive.\n",
    "    \n",
    "Cholesky decomposition\n",
    "    [Cholesky decomposition](cholesky-definition) factorises a positive definite matrix $A$ into a lower triangular matrix $L$ such that $A = LL^\\mathrm{T}$\n",
    "    \\begin{align*}\n",
    "        \\ell_{jj} &= \\sqrt{a_{jj} - \\sum_{k=1}^{j-1} \\ell_{jk}^2 }, & j &= 1, \\ldots, n,\\\\\n",
    "        \\ell_{ij} &= \\dfrac{1}{\\ell_{jj} }\\left(a_{ij} -\\displaystyle \\sum_{k=1}^{i-1} \\ell_{ik} \\ell_{jk} \\right), & i &= j + 1,\\ldots ,n.\n",
    "    \\end{align*}\n",
    "    \n",
    "The Cholesky-Crout method\n",
    "    The solution of a system of linear equations $A \\mathbf{x} = \\mathbf{b}$ where $A$ is positive definite can be calculated using the [Cholesky-Crout's method](cholesky-crout-method-section) where $L \\mathbf{y} = \\mathbf{b}$ is solved for $\\mathbf{y}$ by forward substitution and $L^\\mathrm{T} \\mathbf{x} = \\mathbf{y}$ is solved for $\\mathbf{x}$ by back substitution.\n",
    "    \n",
    "Orthogonal vectors\n",
    "    A set of vectors $\\lbrace \\mathbf{v}_1 ,\\mathbf{v}_2 ,\\mathbf{v}_3 ,\\dots \\rbrace$ is said to be [orthogonal](qr-section) if $\\mathbf{v}_i \\cdot \\mathbf{v}_j =0$ for $i\\not= j$. \n",
    "\n",
    "Orthonormal vectors\n",
    "    A set of orthogonal vectors is [orthonormal](qr-section) if the vectors are all unit vectors.\n",
    "    \n",
    "QR decomposition \n",
    "    [QR decomposition](qr-section) factorises an $m \\times n$ matrix $A$ into an orthogonal matrix $Q$ and an upper triangular matrix $R$ such that $A = QR$.\n",
    "    \n",
    "The Gram-Schmidt process\n",
    "    The [Gram-Schmidt](qr-gram-schmidt-section) process applied to a set of $n$ independent vectors $(\\mathbf{a}_1 ,\\mathbf{a}_2 ,\\dots ,\\mathbf{a}_n)$ produces a set of $n$ orthogonal vectors $(\\mathbf{u}_1 ,\\mathbf{u}_2 ,\\dots ,\\mathbf{u}_n)$ by subtracting the projection of $\\mathbf{a}_i$ onto $\\mathbf{u}_j$ for $j = 1, \\ldots, i = 1$.\n",
    "    \n",
    "QR decomposition using the Gram-Schmidt process\n",
    "    The QR decomposition of a matrix $A$ calculated using the [Gram-Schmidt process](qr-gramschmidt-definition) is\n",
    "\n",
    "    For $j = 1, \\ldots, n$\n",
    "    \\begin{align*}\n",
    "        r_{ij} &=\\mathbf{q}_i \\cdot \\mathbf{a}_j , \\qquad i = 1,\\dots ,j-1,\\\\\n",
    "        \\mathbf{u}_j &= \\mathbf{a}_j - \\sum_{i=1}^{j-1} r_{ij} \\mathbf{q}_i ,\\\\\n",
    "        r_{jj} &= \\| \\mathbf{u}_j \\|,\\\\\n",
    "        \\mathbf{q}_j &=\\frac{\\mathbf{u}_j }{r_{jj}}.\n",
    "    \\end{align*}\n",
    "\n",
    "Crout's method with QR decomposition\n",
    "    The solution of a system of linear equations $A \\mathbf{x} = \\mathbf{b}$ where $A$ is positive definite can be calculated using the [Crout's method with QR decomposition](qr-crout-section) by solving $Rx = Q^\\mathrm{T} \\mathbf{b}$ using back substitution.\n",
    "    \n",
    "Householder transformations\n",
    "    [Householder transformations](qr-householder-section) reflect a vector about a line that passes through the origin.\n",
    "    \n",
    "QR decomposition using Householder transformations\n",
    "    The QR decomposition a matrix $A$ calculated using [Householder transformations](qr-householder-definition) uses the following steps.\n",
    "\n",
    "    For $j = 1, \\ldots, n$\n",
    "\n",
    "    - $\\mathbf{e} = (1, \\underbrace{0, \\ldots, 0}_{m-j})^T$;\n",
    "    - $\\mathbf{u} = (r_{jj}, r_{j+1,j}, \\ldots, r_{m,j})^T$;\n",
    "    - $\\mathbf{u} = \\mathbf{u} + \\operatorname{sign}(r_{jj})\\|\\mathbf{u}\\|\\mathbf{e}$;\n",
    "    - $\\mathbf{v} = \\dfrac{\\mathbf{u}}{\\| \\mathbf{u} \\|}$;\n",
    "    - $H = \\begin{pmatrix}\n",
    "            I_{j-1} & 0 \\\\\n",
    "            0 & I_{m-j+1} - 2 \\mathbf{v}\\mathbf{v}^T\n",
    "        \\end{pmatrix}$;\n",
    "    - $R = H R$;\n",
    "    - $Q = Q H$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f8f0be",
   "metadata": {},
   "source": [
    "---\n",
    "## Indirect methods\n",
    "\n",
    "```{glossary}\n",
    "\n",
    "Indirect method\n",
    "    [Indirect methods](indirect-methods-chapter) is multiple applications of the method to calculate the solution to a system of linear equations. An estimate is made of the solution $\\mathbf{x}^{(0)}$ and an indirect method is applied to calculate the improved estimate $\\mathbf{x}^{(1)}$. The method is applied repeatedly until two successive estimates agree to some tolerance.\n",
    "  \n",
    "Jacobi method\n",
    "    The [Jacobi method](jacobi-method-section) for calculating the solution to a system of linear equations $A \\mathbf{x} = \\mathbf{b}$ is \n",
    "    \\begin{align*}\n",
    "        x_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j = 1}^{i-1} a_{ij} x_j^{(k)} - \\sum_{j = i+1}^n a_{ij} x_j^{(k)} \\right).\n",
    "    \\end{align*}\n",
    "\n",
    "Gauss-Seidel method\n",
    "    The [Gauss-Seidel method](gauss-seidel-method-section) for calculating the solution to a system of linear equations $A \\mathbf{x} = \\mathbf{b}$ is \n",
    "    \\begin{align*}\n",
    "        x_i^{(k+1)} = \\frac{1}{a_{ii} }\\left(b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} -\\sum_{j=i+1}^n a_{ij} x_j^{(k)} \\right).\n",
    "    \\end{align*}\n",
    "    \n",
    "The SOR method\n",
    "    The [SOR method](sor-method-section) for calculating the solution to a system of linear equations $A \\mathbf{x} = \\mathbf{b}$ is\n",
    "    \n",
    "    \\begin{align*}\n",
    "        x_i^{(k+1)} =(1 - \\omega) x_i^{(k)} + \\frac{\\omega}{a_{ii} }\\left(b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} -\\sum_{j=i+1}^n a_{ij} x_j^{(k)} \\right),\n",
    "    \\end{align*}\n",
    "    \n",
    "    where $\\omega \\in [0, 2]$ is a relaxation parameter.\n",
    "    \n",
    "Iteration matrix\n",
    "    The [iteration matrix](indirect-methods-chapter) for a method is the matrix $T$ such that $\\mathbf{x}^{(k+1)} = T\\mathbf{x}^{(k)} + \\mathbf{c}$ where $\\mathbf{x}^{(k)}$ and $\\mathbf{x}^{(k+1)}$ are current and next estimates of the solution. The iteration matrices for the indirect methods are\n",
    "    \n",
    "    \\begin{align*}\n",
    "        \\textsf{Jacobi}: && T_J &= - D^{-1}(L + U),\\\\\n",
    "        \\textsf{Gauss-Seidel}: && T_{GS} &=-(L+D)^{-1} U, \\\\\n",
    "        \\textsf{SOR}: && T_{SOR} &=(D+\\omega L)^{-1} ((1-\\omega )D-\\omega U),\n",
    "    \\end{align*}\n",
    "    \n",
    "    where $A = L + D + U$ and $L$, $D$, and $U$ are lower, diagonal and upper triangular elements of $A$. \n",
    "    \n",
    "Spectral radius\n",
    "    The [spectral radius](spectral-radius-definition) of a matrix is the largest absolute eigenvalue of the matrix.\n",
    "    \n",
    "Convergence of direct methods\n",
    "    A direct method will [converge](convergence-of-indirect-methods-section) to a solution if the spectral radius of the iteration matrix is less than 1. The smaller the spectral radius of the convergence matrix, the faster the convergence.\n",
    "    \n",
    "Optimum value of the relaxation parameter\n",
    "    The [optimum value of the relaxation parameter](optimum-relaxation-parameter-section) for the SOR method is the value of $\\omega$ such that the spectral radius of the iteration matrix is minimised. If the coefficient matrix $A$ has all real eigenvalues then the optimum value of $\\omega$ is\n",
    "    \n",
    "    \\begin{align*}\n",
    "        \\omega_{opt} = 1+{\\left(\\frac{\\rho (T_J )}{1+\\sqrt{1-\\rho (T_J )^2 }}\\right)}^2,\n",
    "    \\end{align*}\n",
    "    \n",
    "    where $\\rho(T_J)$ is the spectral radius of the Jacobi method iteration matrix.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}